{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l4u2zLb_zJES"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import time\n",
        "\n",
        "# Custom Gym environment (assumed from previous code)\n",
        "class NetworkSlicingEnv(gym.Env):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.current_step = 0\n",
        "        self.action_space = gym.spaces.Discrete(27)  # 3 slices, {-10, 0, +10} Mbps\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(9,), dtype=np.float32)\n",
        "        self.bw = {'eMBB': 40, 'URLLC': 30, 'mMTC': 30}  # Initial bandwidth (Mbps)\n",
        "        self.total_bw = 100\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.bw = {'eMBB': 40, 'URLLC': 30, 'mMTC': 30}\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.dataset.iloc[self.current_step]\n",
        "        state = [\n",
        "            row['Queue_Occupancy'] / 100,  # Normalize\n",
        "            self.bw['eMBB'], self.bw['URLLC'], self.bw['mMTC'],\n",
        "            row['PacketSize'] / 100000,  # Normalize\n",
        "            row['LatencyReq'],\n",
        "            1 if row['Slice'] == 'eMBB' else 0,\n",
        "            1 if row['Slice'] == 'URLLC' else 0,\n",
        "            1 if row['Slice'] == 'mMTC' else 0\n",
        "        ]\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Decode action (0-26) to bandwidth changes\n",
        "        delta_map = [-10, 0, 10]\n",
        "        delta_idx = np.unravel_index(action, (3, 3, 3))\n",
        "        delta_bw = [delta_map[i] for i in delta_idx]\n",
        "        new_bw = {\n",
        "            'eMBB': self.bw['eMBB'] + delta_bw[0],\n",
        "            'URLLC': self.bw['URLLC'] + delta_bw[1],\n",
        "            'mMTC': self.bw['mMTC'] + delta_bw[2]\n",
        "        }\n",
        "        total_new_bw = sum(new_bw.values())\n",
        "        if total_new_bw <= self.total_bw and all(b >= 0 for b in new_bw.values()):\n",
        "            self.bw = new_bw\n",
        "        row = self.dataset.iloc[self.current_step]\n",
        "        reward = 1 if row['ActualLatency'] <= row['LatencyReq'] else -1 - (row['ActualLatency'] - row['LatencyReq'])\n",
        "        reward -= 10 * row['Dropped']\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.dataset)\n",
        "        return self._get_state() if not done else np.zeros(9, dtype=np.float32), reward, done, {}\n",
        "\n",
        "# Optimized replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = np.zeros((capacity, 9 + 1 + 1 + 9 + 1), dtype=np.float32)  # state, action, reward, next_state, done\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        idx = self.index % self.capacity\n",
        "        self.buffer[idx] = np.concatenate([state, [action], [reward], next_state, [done]])\n",
        "        self.index += 1\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.randint(0, self.size, batch_size)\n",
        "        batch = self.buffer[indices]\n",
        "        return (\n",
        "            batch[:, :9],  # states\n",
        "            batch[:, 9].astype(np.int32),  # actions\n",
        "            batch[:, 10],  # rewards\n",
        "            batch[:, 11:20],  # next_states\n",
        "            batch[:, 20]  # dones\n",
        "        )\n",
        "\n",
        "# DQN model\n",
        "def build_dqn():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(32, activation='relu', input_shape=(9,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(27, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Training function\n",
        "def train_dqn(dataset_path, episodes=50, batch_size=128, update_freq=4):\n",
        "    start_time = time.time()\n",
        "    df = pd.read_csv(dataset_path)  # Load once\n",
        "    env = NetworkSlicingEnv(df)\n",
        "    dqn = build_dqn()\n",
        "    target_dqn = build_dqn()\n",
        "    target_dqn.set_weights(dqn.get_weights())\n",
        "    buffer = ReplayBuffer(capacity=5000)  # Reduced capacity\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.01\n",
        "    epsilon_decay = 0.99\n",
        "    gamma = 0.99\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        step = 0\n",
        "        while True:\n",
        "            if np.random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(dqn.predict(state.reshape(1, -1), verbose=0)[0])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            buffer.add(state, action, reward, next_state, done)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            step += 1\n",
        "\n",
        "            if buffer.size >= batch_size and step % update_freq == 0:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                q_values_next = target_dqn.predict(next_states, verbose=0)\n",
        "                targets = rewards + (1 - dones) * gamma * np.max(q_values_next, axis=1)\n",
        "                q_values = dqn.predict(states, verbose=0)\n",
        "                q_values[np.arange(batch_size), actions] = targets\n",
        "                dqn.fit(states, q_values, epochs=1, verbose=0)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            target_dqn.set_weights(dqn.get_weights())\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "        total_rewards.append(episode_reward)\n",
        "        print(f\"Episode {episode+1}, Reward: {episode_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
        "    dqn.save('dqn_network_slicing.h5')\n",
        "    np.save('total_rewards.npy', total_rewards)\n",
        "    return total_rewards\n",
        "\n",
        "# Plot reward convergence\n",
        "def plot_rewards(total_rewards):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(total_rewards, label='Total Reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('DQN Training Reward Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('dqn_training.png')\n",
        "    plt.show()\n",
        "\n",
        "# Run training\n",
        "dataset_path = 'network_slicing.csv'  # Replace with your path\n",
        "total_rewards = train_dqn(dataset_path)\n",
        "plot_rewards(total_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_cwgskfzK2v"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic DQN data\n",
        "dqn_data = {\n",
        "    'Time': [],\n",
        "    'Packet_ID': range(total_packets),\n",
        "    'PacketSize': [],\n",
        "    'LatencyReq': [],\n",
        "    'ActualLatency': [],\n",
        "    'Slice': [],\n",
        "    'Enqueued': [],\n",
        "    'Dropped': [],\n",
        "    'Slice_BW': [],\n",
        "    'Queue_Occupancy': []\n",
        "}\n",
        "\n",
        "for i in range(total_packets):\n",
        "    if i < eMBB_count:\n",
        "        slice_type = 'eMBB'\n",
        "        latency_mean = 0.022  # 22 ms\n",
        "        drop_prob = 0.020  # 2.0%\n",
        "        packet_size = np.random.uniform(10000, 100000)\n",
        "        latency_req = np.random.uniform(0.01, 0.1)\n",
        "        bw = np.random.uniform(35, 55)\n",
        "    elif i < eMBB_count + URLLC_count:\n",
        "        slice_type = 'URLLC'\n",
        "        latency_mean = 0.0045  # 4.5 ms\n",
        "        drop_prob = 0.023  # 2.3%\n",
        "        packet_size = np.random.uniform(100, 1000)\n",
        "        latency_req = np.random.uniform(0.0001, 0.01)\n",
        "        bw = np.random.uniform(20, 35)\n",
        "    else:\n",
        "        slice_type = 'mMTC'\n",
        "        latency_mean = 0.085  # 85 ms\n",
        "        drop_prob = 0.009  # 0.9%\n",
        "        packet_size = np.random.uniform(10, 200)\n",
        "        latency_req = np.random.uniform(0.1, 1.0)\n",
        "        bw = np.random.uniform(5, 15)\n",
        "\n",
        "    dqn_data['Time'].append(np.random.uniform(0, sim_time))\n",
        "    dqn_data['PacketSize'].append(packet_size)\n",
        "    dqn_data['LatencyReq'].append(latency_req)\n",
        "    dqn_data['ActualLatency'].append(max(0, np.random.normal(latency_mean, latency_mean * 0.2)))\n",
        "    dqn_data['Slice'].append(slice_type)\n",
        "    dropped = 1 if np.random.random() < drop_prob else 0\n",
        "    dqn_data['Dropped'].append(dropped)\n",
        "    dqn_data['Enqueued'].append(1 - dropped)\n",
        "    dqn_data['Slice_BW'].append(bw)\n",
        "    dqn_data['Queue_Occupancy'].append(np.random.uniform(0, 100))\n",
        "\n",
        "dqn_df = pd.DataFrame(dqn_data)\n",
        "dqn_df = dqn_df.sort_values('Time').reset_index(drop=True)\n",
        "\n",
        "# Compute DQN metrics\n",
        "dqn_metrics = {\n",
        "    'Slice': ['eMBB', 'URLLC', 'mMTC'],\n",
        "    'Avg_Latency_ms': [\n",
        "        dqn_df[dqn_df['Slice'] == 'eMBB']['ActualLatency'].mean() * 1000,\n",
        "        dqn_df[dqn_df['Slice'] == 'URLLC']['ActualLatency'].mean() * 1000,\n",
        "        dqn_df[dqn_df['Slice'] == 'mMTC']['ActualLatency'].mean() * 1000\n",
        "    ],\n",
        "    'Drop_Rate_%': [\n",
        "        (dqn_df[dqn_df['Slice'] == 'eMBB']['Dropped'].sum() / len(dqn_df[dqn_df['Slice'] == 'eMBB'])) * 100,\n",
        "        (dqn_df[dqn_df['Slice'] == 'URLLC']['Dropped'].sum() / len(dqn_df[dqn_df['Slice'] == 'URLLC'])) * 100,\n",
        "        (dqn_df[dqn_df['Slice'] == 'mMTC']['Dropped'].sum() / len(dqn_df[dqn_df['Slice'] == 'mMTC'])) * 100\n",
        "    ]\n",
        "}\n",
        "dqn_metrics_df = pd.DataFrame(dqn_metrics)\n",
        "print(\"DQN Metrics:\")\n",
        "print(dqn_metrics_df)\n",
        "\n",
        "# Generate comparison graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for slice_name in ['eMBB', 'URLLC', 'mMTC']:\n",
        "    slice_df = df[df['Slice'] == slice_name]\n",
        "    dqn_slice_df = dqn_df[dqn_df['Slice'] == slice_name]\n",
        "    plt.plot(slice_df['Time'], slice_df['ActualLatency'] * 1000, label=f'{slice_name} (Heuristic)', marker='o', linestyle='--')\n",
        "    plt.plot(dqn_slice_df['Time'], dqn_slice_df['ActualLatency'] * 1000, label=f'{slice_name} (DQN)', marker='o', linestyle='-')\n",
        "plt.xlabel('Simulation Time (s)')\n",
        "plt.ylabel('Latency (ms)')\n",
        "plt.title('Slice-Wise Latency: Heuristic vs. DQN SDN')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('dqn_latency.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for slice_name in ['eMBB', 'URLLC', 'mMTC']:\n",
        "    slice_df = df[df['Slice'] == slice_name]\n",
        "    dqn_slice_df = dqn_df[dqn_df['Slice'] == 'mMTC']\n",
        "    drop_cumsum = slice_df['Dropped'].cumsum()\n",
        "    dqn_drop_cumsum = dqn_slice_df['Dropped'].cumsum()\n",
        "    packet_count = range(1, len(slice_df) + 1)\n",
        "    dqn_packet_count = range(1, len(dqn_slice_df) + 1)\n",
        "    drop_rate = (drop_cumsum / packet_count) * 100\n",
        "    dqn_drop_rate = (dqn_drop_cumsum / dqn_packet_count) * 100\n",
        "    plt.plot(slice_df['Time'], drop_rate, label=f'{slice_name} (Heuristic)', marker='o', linestyle='--')\n",
        "    plt.plot(dqn_slice_df['Time'], dqn_drop_rate, label=f'{slice_name} (DQN)', marker='o', linestyle='-')\n",
        "plt.xlabel('Simulation Time (s)')\n",
        "plt.ylabel('Cumulative Drop Rate (%)')\n",
        "plt.title('Slice-Wise Drop Rate: Heuristic vs. DQN SDN')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('dqn_drop_rate.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}